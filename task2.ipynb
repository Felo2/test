import numpy as np 
import pandas as pd
data = pd.read_csv('kddcup.data_10_percent.csv', header=None)
data.columns = [
    'duration',
    'protocol_type',
    'service',
    'flag',
    'src_bytes',
    'dst_bytes',
    'land',
    'wrong_fragment',
    'urgent',
    'hot',
    'num_failed_logins',
    'logged_in',
    'num_compromised',
    'root_shell',
    'su_attempted',
    'num_root',
    'num_file_creations',
    'num_shells',
    'num_access_files',
    'num_outbound_cmds',
    'is_host_login',
    'is_guest_login',
    'count',
    'srv_count',
    'serror_rate',
    'srv_serror_rate',
    'rerror_rate',
    'srv_rerror_rate',
    'same_srv_rate',
    'diff_srv_rate',
    'srv_diff_host_rate',
    'dst_host_count',
    'dst_host_srv_count',
    'dst_host_same_srv_rate',
    'dst_host_diff_srv_rate',
    'dst_host_same_src_port_rate',
    'dst_host_srv_diff_host_rate',
    'dst_host_serror_rate',
    'dst_host_srv_serror_rate',
    'dst_host_rerror_rate',
    'dst_host_srv_rerror_rate',
    'outcome'
]
#data.sample(10)

print(data['num_outbound_cmds'].unique())


# LET US DO SOME CLEANING UP OF THE DATA

# this will determine the numeric zscore
def encode_numeric_zscore(df, name, mean=None, sd=None):
    if mean is None:
        mean = df[name].mean()

    if sd is None:
        sd = df[name].std()

    df[name] = (df[name] - mean) / sd
    
# encoding text dummy
def encode_text_dummy(df, name):
    dummies = pd.get_dummies(df[name])
    for x in dummies.columns:
        dummy_name = f"{name}-{x}"
        df[dummy_name] = dummies[x]
    df.drop(name, axis=1, inplace=True)
# using our functions
encode_numeric_zscore(data, 'duration')
encode_text_dummy(data, 'protocol_type')
encode_text_dummy(data, 'service')
encode_text_dummy(data, 'flag')
encode_numeric_zscore(data, 'src_bytes')
encode_numeric_zscore(data, 'dst_bytes')
encode_text_dummy(data, 'land')
encode_numeric_zscore(data, 'wrong_fragment')
encode_numeric_zscore(data, 'urgent')
encode_numeric_zscore(data, 'hot')
encode_numeric_zscore(data, 'num_failed_logins')
encode_text_dummy(data, 'logged_in')
encode_numeric_zscore(data, 'num_compromised')
encode_numeric_zscore(data, 'root_shell')
encode_numeric_zscore(data, 'su_attempted')
encode_numeric_zscore(data, 'num_root')
encode_numeric_zscore(data, 'num_file_creations')
encode_numeric_zscore(data, 'num_shells')
encode_numeric_zscore(data, 'num_access_files')
encode_numeric_zscore(data, 'num_outbound_cmds')
encode_text_dummy(data, 'is_host_login')
encode_text_dummy(data, 'is_guest_login')
encode_numeric_zscore(data, 'count')
encode_numeric_zscore(data, 'srv_count')
encode_numeric_zscore(data, 'serror_rate')
encode_numeric_zscore(data, 'srv_serror_rate')
encode_numeric_zscore(data, 'rerror_rate')
encode_numeric_zscore(data, 'srv_rerror_rate')
encode_numeric_zscore(data, 'same_srv_rate')
encode_numeric_zscore(data, 'diff_srv_rate')
encode_numeric_zscore(data, 'srv_diff_host_rate')
encode_numeric_zscore(data, 'dst_host_count')
encode_numeric_zscore(data, 'dst_host_srv_count')
encode_numeric_zscore(data, 'dst_host_same_srv_rate')
encode_numeric_zscore(data, 'dst_host_diff_srv_rate')
encode_numeric_zscore(data, 'dst_host_same_src_port_rate')
encode_numeric_zscore(data, 'dst_host_srv_diff_host_rate')
encode_numeric_zscore(data, 'dst_host_serror_rate')
encode_numeric_zscore(data, 'dst_host_srv_serror_rate')
encode_numeric_zscore(data, 'dst_host_rerror_rate')
encode_numeric_zscore(data, 'dst_host_srv_rerror_rate')

# drop Null values
data.dropna(inplace=True, axis=1)
data.outcome.value_counts()

x_columns = data.columns.drop('outcome')
x = data[x_columns].values
dummies = pd.get_dummies(data['outcome'])
outcomes = dummies.columns
num_classes = len(outcomes)
y = dummies.values
# importing the trains test split
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=51)
# this is how we use tensorflow in Artificial Neural Networks

import tensorflow
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, ELU, Input, Dropout

input = Input(shape=x.shape[1])

m = Dense(64)(input) 
m = ELU()(m) #
m = Dropout(0.33)(m) 

m = Dense(64)(m)
m = ELU()(m)
m = Dropout(0.33)(m)

m = Dense(32)(m)
m = ELU()(m)
m = Dropout(0.33)(m)

m = Dense(16)(m)
m = ELU()(m)
m = Dropout(0.33)(m)

output = Dense(y.shape[1], activation='softmax')(m)

model = Model(inputs=[input], outputs=[output])

model.summary()


# additional steps to optimize our model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['acc'])

es = EarlyStopping(monitor='val_loss', patience=30, verbose=1, restore_best_weights=True)

rlp = ReduceLROnPlateau(monitor='val_loss', patience=9, verbose=1, factor=0.5, cooldown=5, min_lr=1e-10)
# this will take a bit of time since we have 30 Epochs
history = model.fit(x_train
                    ,y_train
                    ,callbacks=[es, rlp]
                    ,verbose=1
                    ,epochs=30
                    , batch_size=512).history


# matplotlib and seaborn are data visualization packages
import matplotlib.pyplot as plt
import seaborn as sns

# let us plot the accuracy and loss of our model
fig, (ax1, ax2) = plt.subplots(2, 1, sharex='col', figsize=(20, 14))

ax1.plot(history['loss'], label='Train loss')
ax1.legend(loc='best')
ax1.set_title('Loss')

# history['acc'] has the accuracy of our model
ax2.plot(history['acc'], label='Train accuracy')
ax2.legend(loc='best')
ax2.set_title('Accuracy')

plt.xlabel('Epochs')
sns.despine()
plt.show()

# finally, let us try and evaluate the performance of our model
model.evaluate(x_test, y_test)

# the acc: 0.9999 which is VERY VERY GOOD!

